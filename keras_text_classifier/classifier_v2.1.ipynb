{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import itertools\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_hub as hub\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Embedding, Conv1D, GlobalMaxPooling1D, Flatten, LSTM, \\\n",
    "Bidirectional, CuDNNLSTM\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import utils\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = pd.read_csv(\"../data/train_with_cname.csv\")\n",
    "testData = pd.read_csv(\"../data/test.csv\")\n",
    "dictData = pd.read_csv(\"../data/kata_dasar_kbbi.csv\")\n",
    "categories_file = open(\"../data/categories.json\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_file = open('../data/glove.840B.300d.txt', \"r\", encoding=\"Latin-1\")\n",
    "embeddings_index = {}\n",
    "for line in glove_file:\n",
    "    values = line.split()\n",
    "    word = ''.join(values[:-300])\n",
    "    coefs = np.asarray(values[-300:], dtype='float32')\n",
    "    # print(coefs)\n",
    "    embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = json.load(categories_file)\n",
    "inverted_categories_mobile = {v: k.lower() for k, v in categories['Mobile'].items()}\n",
    "inverted_categories_fashion = {v: k.lower() for k, v in categories['Fashion'].items()}\n",
    "inverted_categories_beauty = {v: k.lower() for k, v in categories['Beauty'].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subcategories = {k.lower(): v for k, v in categories['Mobile'].items()}\n",
    "all_subcategories.update({k.lower(): v for k, v in categories['Fashion'].items()})\n",
    "all_subcategories.update({k.lower(): v for k, v in categories['Beauty'].items()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main settings\n",
    "plot_history_check = True\n",
    "gen_test = False\n",
    "max_length = 35  # 32 is max word in train\n",
    "max_words = 1000\n",
    "num_classes = len(all_subcategories)\n",
    "# Training for more epochs will likelval-acc after 10 epochs: 0.71306y lead to overfitting on this dataset\n",
    "# You can try tweaking these hyperparamaters when using this model with your own data\n",
    "batch_size = 256\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'others mobile & tablet': 35, 'smartfren': 53, 'infinix': 40, 'brandcode': 39, 'icherry': 52, 'advan': 45, 'iphone': 31, 'realme': 51, 'motorola': 49, 'maxtron': 56, 'nokia': 38, 'xiaomi': 34, 'mito': 46, 'sony': 33, 'spc': 57, 'lenovo': 37, 'alcatel': 55, 'samsung': 32, 'vivo': 42, 'evercoss': 44, 'strawberry': 50, 'blackberry': 36, 'asus': 43, 'honor': 54, 'oppo': 41, 'huawei': 47, 'sharp': 48, 'wedding dress': 23, 'shirt': 27, 'casual dress': 18, 'maxi dress': 20, 'big size dress': 24, 'bodycon dress': 22, 'party dress': 19, 'blouse': 26, 'tshirt': 25, 'crop top': 29, 'tanktop': 28, 'others': 17, 'a line dress': 21, 'big size top': 30, 'foundation': 1, 'face palette': 0, 'concealer': 7, 'lip gloss': 14, 'blush on': 2, 'highlighter': 8, 'bb & cc cream': 5, 'other face cosmetics': 4, 'lip tint': 13, 'bronzer': 11, 'lip liner': 15, 'powder': 3, 'setting spray': 10, 'primer': 9, 'contour': 6, 'other lip cosmetics': 16, 'lipstick': 12}\n",
      "no of categories: 58\n"
     ]
    }
   ],
   "source": [
    "print(all_subcategories)\n",
    "print(\"no of categories: \" + str(num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_mapping = {\n",
    "    'fashion_image': 'Fashion',\n",
    "    'beauty_image': 'Beauty',\n",
    "    'mobile_image': 'Mobile',\n",
    "}\n",
    "directory_mapping = {\n",
    "    'Fashion': 'fashion_image',\n",
    "    'Beauty': 'beauty_image',\n",
    "    'Mobile': 'mobile_image',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle train data\n",
    "trainData = shuffle(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "633284 666615\n"
     ]
    }
   ],
   "source": [
    "max_data_size = int(len(trainData) * 1)\n",
    "train_data_size = int(max_data_size * .95)\n",
    "train_data_step = 1\n",
    "validate_data_step = 1\n",
    "print(train_data_size, max_data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "666615 666615\n"
     ]
    }
   ],
   "source": [
    "train_texts = trainData['title'][::train_data_step]\n",
    "train_tags = trainData['Category'][::train_data_step]\n",
    "test_texts = testData['title']\n",
    "print(len(train_texts), len(train_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize = text.Tokenizer(num_words=1000, char_level=False)\n",
    "tokenize.fit_on_texts(train_texts)  # only fit on train\n",
    "word_index = tokenize.word_index\n",
    "x_train = tokenize.texts_to_sequences(train_texts)\n",
    "x_test = tokenize.texts_to_sequences(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences with zeros\n",
    "x_train = pad_sequences(x_train, padding='post', maxlen=max_length)\n",
    "x_test = pad_sequences(x_test, padding='post', maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_tags.values\n",
    "y_train = utils.to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 35, 300)           24027600  \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 35, 256)           440320    \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 256)               395264    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 58)                14906     \n",
      "=================================================================\n",
      "Total params: 24,943,882\n",
      "Trainable params: 24,943,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                    300,\n",
    "                    input_length=max_length,\n",
    "                    weights=[embedding_matrix],\n",
    "                    trainable=True))\n",
    "model.add(Bidirectional(CuDNNLSTM(128, return_sequences=True)))\n",
    "model.add(Bidirectional(CuDNNLSTM(128)))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_filename_h5():\n",
    "    return 'epoch_'+str(epochs) + '_' + datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_filename_csv():\n",
    "    return 'epoch_'+str(epochs) + '_' + datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint auto\n",
    "filepath = \"../checkpoints/\"+gen_filename_h5()+\"v2.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(666615, 35)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(666615,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 599953 samples, validate on 66662 samples\n",
      "Epoch 1/10\n",
      "599953/599953 [==============================] - 83s 139us/step - loss: 1.1255 - acc: 0.6659 - val_loss: 0.9625 - val_acc: 0.6994\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.72440\n",
      "Epoch 2/10\n",
      "599953/599953 [==============================] - 82s 136us/step - loss: 0.9677 - acc: 0.7015 - val_loss: 0.9170 - val_acc: 0.7104\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.72440\n",
      "Epoch 3/10\n",
      "599953/599953 [==============================] - 83s 138us/step - loss: 0.9193 - acc: 0.7130 - val_loss: 0.8986 - val_acc: 0.7150\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.72440\n",
      "Epoch 4/10\n",
      "599953/599953 [==============================] - 81s 134us/step - loss: 0.8807 - acc: 0.7224 - val_loss: 0.8779 - val_acc: 0.7217\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.72440\n",
      "Epoch 5/10\n",
      "599953/599953 [==============================] - 81s 135us/step - loss: 0.8465 - acc: 0.7315 - val_loss: 0.8724 - val_acc: 0.7228\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.72440\n",
      "Epoch 6/10\n",
      "599953/599953 [==============================] - 82s 136us/step - loss: 0.8146 - acc: 0.7398 - val_loss: 0.8683 - val_acc: 0.7281\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.72440 to 0.72809, saving model to ../checkpoints/epoch_10_03_04_2019_00_24_51v2.hdf5\n",
      "Epoch 7/10\n",
      "599953/599953 [==============================] - 81s 135us/step - loss: 0.7857 - acc: 0.7477 - val_loss: 0.8689 - val_acc: 0.7282\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.72809 to 0.72820, saving model to ../checkpoints/epoch_10_03_04_2019_00_24_51v2.hdf5\n",
      "Epoch 8/10\n",
      "599953/599953 [==============================] - 81s 135us/step - loss: 0.7568 - acc: 0.7555 - val_loss: 0.8733 - val_acc: 0.7289\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.72820 to 0.72887, saving model to ../checkpoints/epoch_10_03_04_2019_00_24_51v2.hdf5\n",
      "Epoch 9/10\n",
      "599953/599953 [==============================] - 81s 134us/step - loss: 0.7313 - acc: 0.7624 - val_loss: 0.8827 - val_acc: 0.7291\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.72887 to 0.72914, saving model to ../checkpoints/epoch_10_03_04_2019_00_24_51v2.hdf5\n",
      "Epoch 10/10\n",
      "542976/599953 [==========================>...] - ETA: 7s - loss: 0.7041 - acc: 0.7698"
     ]
    }
   ],
   "source": [
    "history = model.fit([x_train], batch_size=batch_size, y=y_train, verbose=1, validation_split=0.1,\n",
    "                    shuffle=True, epochs=epochs, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    plt.style.use('ggplot')\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_test():\n",
    "    prediction = model.predict(x_test, batch_size=batch_size, verbose=1)\n",
    "    predicted_label = [np.argmax(prediction[i]) for i in range(len(x_test))]\n",
    "    # print(predicted_label)\n",
    "    df = pd.DataFrame({'itemid': testData['itemid'].astype(int), 'Category': predicted_label})\n",
    "    df.to_csv(path_or_buf='res_' + gen_filename_csv() + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
