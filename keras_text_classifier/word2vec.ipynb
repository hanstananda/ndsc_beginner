{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "56f50e95b245e7041cda2f292062fbc926f5f5ac"
   },
   "source": [
    "# **How to train your <del>dragon</del> custom word embeddings**\n",
    "\n",
    "In the [baseline-keras-lstm-is-all-you-need](https://www.kaggle.com/huikang/baseline-keras-lstm-is-all-you-need) notebook shared by Hui Kang (thanks again!), it was demonstrated that a LSTM model using generic global vector (GLOVE) achieved a pretty solid benchmark results.\n",
    "\n",
    "After playing around with GLOVE, you will quickly find that certain words in your training data are not present in its vocab. These are typically replaced with same-shape zero vector, which essentially means you are 'sacrificing' the word as your input feature, which can potentially be important for correct prediction. Another way to deal with this is to train your own word embeddings, using your training data, so that the semantic relationship of your own training corpus can be better represented.\n",
    "\n",
    "In this notebook, I will demonstrate how to train your custom word2vec using Gensim.\n",
    "\n",
    "For those who are new to word embeddings and would like to find out more, you can check out the following articles:\n",
    "1. [Introduction to Word Embedding and Word2Vec](https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa)\n",
    "2. [A Beginner's Guide to Word2Vec and Neural Word Embeddings](https://skymind.ai/wiki/word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "821183ea8057c78e9e3280801e5b80329d248ada"
   },
   "source": [
    "## **Something to take note**\n",
    "Word2vec is a **self-supervised** method (well, sort of unsupervised but not unsupervised, since it provides its own labels. check out this [Quora](https://www.quora.com/Is-Word2vec-a-supervised-unsupervised-learning-algorithm) thread for a more detailed explanation), so we can make full use of the entire dataset (including test data) to obtain a more wholesome word embedding representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "dd627323f593ee323a3ae62954bc06ccceca485d"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/new_train.csv')\n",
    "df_test = pd.read_csv('../data/new_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "eefec6cadf21909d078915466e3fd672d62f4f0e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 839017/839017 [00:01<00:00, 568440.59it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences = pd.concat([df_train['title'], df_test['title']],axis=0)\n",
    "train_sentences = list(sentences.progress_apply(str.split).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "e1acc9328db3e49a478a605c27ceb4886e69513e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken : 0.81 mins\n"
     ]
    }
   ],
   "source": [
    "# Parameters reference : https://www.quora.com/How-do-I-determine-Word2Vec-parameters\n",
    "# Feel free to customise your own embedding\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model = Word2Vec(sentences=train_sentences, \n",
    "                 sg=1, \n",
    "                 size=300,\n",
    "                 workers=4)\n",
    "\n",
    "print(f'Time taken : {(time.time() - start_time) / 60:.2f} mins')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e6820a6e76f3ed9f8386c4eddcdbeb9fb45e6e6b"
   },
   "source": [
    "## **Pretty fast isn't it.**\n",
    "\n",
    "Let's check out some of the features of the customised word vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "f4cc2a2eb6e1a836b2fb39bdd1f1572ebde0517d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15702"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total number of vocab in our custom word embedding\n",
    "\n",
    "len(model.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "f288af1fc4d32841e948759840296956167ce78e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out the dimension of each word (we set it to 300 in the above training step)\n",
    "\n",
    "model.wv.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "7ddfb22a1b2f0ef3a7b167580cc7abc324643730"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.5451705 ,  0.17535159, -0.46454248, -0.17189127,  0.5599154 ,\n",
       "       -0.5107339 ,  0.16901785, -0.18851307,  0.14714868,  0.09982701,\n",
       "       -0.08246141, -0.17406596,  0.2804124 , -0.0507641 , -0.48802358,\n",
       "       -0.36542034, -0.2145641 , -0.04688673,  0.1424934 ,  0.1637054 ,\n",
       "       -0.27545628,  0.17334777,  0.2657709 , -0.19411126, -0.04116624,\n",
       "       -0.49339792, -0.46244827, -0.05299556, -0.42241699,  0.03331193,\n",
       "        0.13610442,  0.29399866, -0.02095415,  0.0515291 ,  0.32896805,\n",
       "       -0.11736588, -0.2722362 ,  0.43094817,  0.05486928, -0.31320786,\n",
       "        0.02036065,  0.15312116, -0.36650494,  0.4689258 ,  0.7094724 ,\n",
       "       -0.08336945, -0.19291283, -0.68246174, -0.02891858,  0.57406276,\n",
       "        0.2672893 ,  0.1489239 ,  0.08525643,  0.05862563, -0.20578611,\n",
       "        0.02702558, -0.23098549,  0.33465037, -0.13116418,  0.5667268 ,\n",
       "        0.05880717,  0.15229182,  0.23845781,  0.61248153, -0.2200646 ,\n",
       "       -0.256163  , -0.0071689 ,  0.12713589,  0.09662355, -0.2792842 ,\n",
       "        0.17807959, -0.20650457,  0.78325576,  0.1948286 , -0.07940456,\n",
       "       -0.12510109,  0.13296343,  0.05120348, -0.33862564,  0.16319895,\n",
       "        0.3403596 ,  0.05110459, -0.01957047, -0.0737574 ,  0.0743013 ,\n",
       "       -0.04490487, -0.09272285,  0.2323862 , -0.1479468 , -0.105235  ,\n",
       "        0.39412403, -0.002401  , -0.3424335 ,  0.09544878, -0.42229933,\n",
       "        0.05203407,  0.14215915, -0.15745085,  0.2261525 , -0.48303208,\n",
       "        0.25610328, -0.2574822 , -0.10280202, -0.12177072,  0.5912794 ,\n",
       "       -0.37787247,  0.01785006, -0.2798216 ,  0.01529921,  0.42465106,\n",
       "        0.1127046 , -0.09460894, -0.13295892, -0.35919836, -0.1421548 ,\n",
       "       -0.45904836, -0.22982073, -0.36315882, -0.10742419,  0.05900837,\n",
       "       -0.10616069,  0.34501696,  0.11143143, -0.18763997,  0.09558543,\n",
       "       -0.00235726, -0.0488565 ,  0.0957864 ,  0.33521432,  0.04294782,\n",
       "       -0.0268249 , -0.12111954,  0.11146745,  0.11259415,  0.00544581,\n",
       "        0.09501063,  0.12854396, -0.5496415 ,  0.01672476, -0.11612565,\n",
       "        0.08011827, -0.255972  , -0.38329527, -0.12071744, -0.0241557 ,\n",
       "        0.20781279, -0.08005866,  0.14222664, -0.17649798, -0.04768788,\n",
       "        0.18021788, -0.40773022,  0.78021514, -0.13176028,  0.23580171,\n",
       "       -0.45756912, -0.4519431 , -0.2452348 , -0.12174142, -0.5029048 ,\n",
       "        0.32824257, -0.0716834 , -0.22472496, -0.29370594, -0.20656815,\n",
       "       -0.27403486,  0.16123348, -0.44555685, -0.6279058 , -0.14934266,\n",
       "       -0.30809915,  0.01775396,  0.45507354,  0.16559322, -0.6568466 ,\n",
       "       -0.05192303, -0.14017446,  0.15519038,  0.07614025, -0.39795083,\n",
       "        0.45767406,  0.14105146, -0.08717077,  0.19673981,  0.22590396,\n",
       "        0.13565554,  0.2672275 ,  0.01721166, -0.22433889,  0.10439025,\n",
       "       -0.03380212,  0.08786635, -0.55204505, -0.13065442, -0.2881389 ,\n",
       "       -0.31241465,  0.3583113 ,  0.05556862,  0.06558836, -0.49807364,\n",
       "       -0.27877605,  0.09859524,  0.27588323,  0.21353364, -0.15581419,\n",
       "        0.11584803, -0.34165174, -0.2930298 , -0.39705962, -0.09507212,\n",
       "        0.7756436 ,  0.04075352,  0.21032414, -0.19664042, -0.12397064,\n",
       "       -0.33455452, -0.14949913,  0.07002061,  0.10548643,  0.0312336 ,\n",
       "       -0.16632281,  0.19763063,  0.0385809 , -0.09125707, -0.06588952,\n",
       "        0.11213291, -0.00624048, -0.3216186 ,  0.28503472,  0.0528853 ,\n",
       "        0.05555345, -0.08989651, -0.3637592 , -0.09499572,  0.11600545,\n",
       "        0.21515466, -0.04730604,  0.22584498,  0.21335554, -0.07636415,\n",
       "       -0.17906123,  0.00580303, -0.39703953,  0.26839212,  0.09827827,\n",
       "       -0.01711191, -0.41546062, -0.17647241,  0.12742335,  0.17002422,\n",
       "        0.24625751,  0.47410467,  0.10644247, -0.24547794,  0.16401652,\n",
       "        0.4769005 , -0.41512367,  0.24138127, -0.02212352,  0.17669234,\n",
       "        0.16722794,  0.08071616, -0.00917166,  0.3579524 , -0.20282146,\n",
       "       -0.26165134, -0.1276496 ,  0.02915077,  0.02199289,  0.17309524,\n",
       "        0.27141953,  0.11926858,  0.49048245, -0.09138735,  0.3434742 ,\n",
       "        0.35161614, -0.11576173, -0.20807113, -0.08738516, -0.5726559 ,\n",
       "       -0.00086473,  0.30186692,  0.12245179,  0.06903422,  0.38142338,\n",
       "        0.5622804 , -0.11639154,  0.36166582, -0.51108956,  0.13941626,\n",
       "       -0.05221739, -0.24004915,  0.2870664 ,  0.40461615, -0.3087322 ,\n",
       "        0.28075325, -0.29829347, -0.40093547, -0.09387563,  0.10527915],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out how 'iphone' is represented (an array of 100 numbers)\n",
    "\n",
    "model.wv.get_vector('iphone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "465e6d2e4ff285001317589a5abcc0a7a64607aa"
   },
   "source": [
    "## Now, why are word embeddings powerful? \n",
    "\n",
    "This is because they capture the semantics relationships between words. In other words, words with similar meanings should appear near each other in the vector space of our custom embeddings.\n",
    "\n",
    "Lets check out an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "3dfbc936126bc2fa261b244fb2ffca4c88887ba7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('jetblack', 0.637566089630127),\n",
       " ('spacegrey', 0.6348098516464233),\n",
       " ('originaliphone', 0.6347168684005737),\n",
       " ('splus', 0.6323285102844238),\n",
       " ('selleriphone', 0.6306023001670837),\n",
       " ('apple', 0.629304051399231),\n",
       " ('iph', 0.6253551244735718),\n",
       " ('cpo', 0.6204924583435059),\n",
       " ('singapura', 0.6125050783157349),\n",
       " ('mateblack', 0.6120768189430237)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find words with similar meaning to 'iphone'\n",
    "\n",
    "model.wv.most_similar('iphone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "686e58aee82c64f9602fecc97fe554c031c2ce9a"
   },
   "source": [
    "Well, you will see words similar to 'iphone', sorted based on euclidean distance.\n",
    "Of cause, there are also not so intuitive and relevant ones (e.g. jetblack, cpo, ten). If you would like to tackle this, you can do a more thorough pre-processing/ try other embedding dimensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d18e90fe3baf47224ec8452c853916d514b9a3ad"
   },
   "source": [
    "## **The most important part!**\n",
    "Last but not least, save your word embeddings, so that you can used it for modelling. You can load the text file next time using Gensim KeyedVector function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "617b3112a72305368bd235290fe393bc16577fbf"
   },
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format('custom_glove_300d.txt')\n",
    "\n",
    "\n",
    "# How to load:\n",
    "# w2v = KeyedVectors.load_word2vec_format('custom_glove_100d.txt')\n",
    "\n",
    "# How to get vector using loaded model\n",
    "# w2v.get_vector('iphone')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2f272c0a251aadb452474dfb7e2139476497e8fd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
