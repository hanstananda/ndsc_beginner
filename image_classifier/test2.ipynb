{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "from keras import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Activation, Dropout, Embedding, Conv1D, GlobalMaxPooling1D, Bidirectional, CuDNNLSTM, \\\n",
    "    SpatialDropout1D, MaxPooling1D,Conv2D,MaxPooling2D,Flatten\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utility.train_data_loader import load_train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom train data used\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hans Tananda\\.virtualenvs\\ndsc-beginner\\lib\\site-packages\\ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 256\n",
    "specialization = \"fashion\"\n",
    "gen_test = True\n",
    "\n",
    "categories_file = open(\"../data/categories.json\", \"r\")\n",
    "categories = json.load(categories_file)\n",
    "\n",
    "all_subcategories = {k.lower(): v for k, v in categories['Mobile'].items()}\n",
    "all_subcategories.update({k.lower(): v for k, v in categories['Fashion'].items()})\n",
    "all_subcategories.update({k.lower(): v for k, v in categories['Beauty'].items()})\n",
    "\n",
    "data_root = \"../../\"+specialization+\"_image/\"\n",
    "\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "valid_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "feature_extractor_url = \"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/2\"\n",
    "\n",
    "trainData = load_train_data()\n",
    "testData = pd.read_csv(\"../data/test.csv\")\n",
    "\n",
    "validation_data_specialized = trainData[trainData['image_path'].str.contains(specialization)][::100]\n",
    "validation_data_specialized['image_path'] = validation_data_specialized['image_path']. \\\n",
    "    map(lambda x: x.replace(specialization + '_image/', ''))\n",
    "\n",
    "test_data_specialized = testData[testData['image_path'].str.contains(specialization)]\n",
    "test_data_specialized['image_path'] = test_data_specialized['image_path'].\\\n",
    "    map(lambda x: x.replace(specialization+'_image/', ''))\n",
    "\n",
    "inverted_categories_specialized = {k.lower(): v for k, v in categories[specialization.capitalize()].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1503 167\n",
      "2000 200\n",
      "2000 200\n",
      "2000 200\n",
      "2000 200\n",
      "2000 200\n",
      "2000 200\n",
      "2000 200\n",
      "2000 200\n",
      "2000 200\n",
      "2000 200\n",
      "2000 200\n",
      "2000 200\n",
      "1296 144\n"
     ]
    }
   ],
   "source": [
    "train_data_specialized = trainData[trainData['image_path'].str.contains(specialization)][::]\n",
    "df_train = pd.DataFrame()\n",
    "df_valid = pd.DataFrame()\n",
    "num_train=2000\n",
    "num_valid=int(0.1*num_train)\n",
    "for k,v in inverted_categories_specialized.items():\n",
    "    rows = train_data_specialized.loc[train_data_specialized['Category'] == v]\n",
    "    num_images = rows.shape[0]\n",
    "    if(num_train+num_valid>num_images):\n",
    "        nt=int(0.9*num_images)\n",
    "        nv=int(0.1*num_images)\n",
    "    else:\n",
    "        nt=num_train\n",
    "        nv=num_valid\n",
    "    # print(nt,nv)\n",
    "    rows_train = rows[:nt]\n",
    "    df_train = df_train.append(rows_train)\n",
    "    rows_valid = rows[nt:(nt+num_valid)]\n",
    "    df_valid = df_valid.append(rows_valid)\n",
    "\n",
    "train_data_specialized = df_train\n",
    "validation_data_specialized = df_valid\n",
    "\n",
    "train_data_specialized['image_path'] = train_data_specialized['image_path']. \\\n",
    "    map(lambda x: x.replace(specialization + '_image/', ''))\n",
    "\n",
    "validation_data_specialized['image_path'] = validation_data_specialized['image_path']. \\\n",
    "    map(lambda x: x.replace(specialization + '_image/', ''))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26799 images belonging to 14 classes.\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = hub.get_expected_image_size(hub.Module(feature_extractor_url))\n",
    "image_generator = datagen.flow_from_dataframe(train_data_specialized,\n",
    "                                              directory=os.path.join(data_root),\n",
    "                                              x_col=\"image_path\",\n",
    "                                              y_col=\"item_category\",\n",
    "                                              target_size=IMAGE_SIZE,\n",
    "                                              color_mode=\"rgb\",\n",
    "                                              class_mode=\"categorical\",\n",
    "                                              shuffle=True,\n",
    "                                              batch_size=64,\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch shape:  (64, 224, 224, 3)\n",
      "Label batch shape:  (64, 14)\n"
     ]
    }
   ],
   "source": [
    "label_names = sorted(image_generator.class_indices.items(), key=lambda pair:pair[1])\n",
    "label_names = np.array([key.title() for key, value in label_names])\n",
    "\n",
    "\n",
    "def feature_extractor(x):\n",
    "    feature_extractor_module = hub.Module(feature_extractor_url)\n",
    "    return feature_extractor_module(x)\n",
    "\n",
    "\n",
    "for image_batch, label_batch in image_generator:\n",
    "    print(\"Image batch shape: \", image_batch.shape)\n",
    "    print(\"Label batch shape: \", label_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 222, 222, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 111, 111, 32)      0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 111, 111, 32)      0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 394272)            0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               50466944  \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 14)                1806      \n",
      "=================================================================\n",
      "Total params: 50,469,646\n",
      "Trainable params: 50,469,646\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = IMAGE_SIZE+[3] \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(inverted_categories_specialized), activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2713 images belonging to 14 classes.\n",
      "Found 55440 images.\n"
     ]
    }
   ],
   "source": [
    "valid_generator = valid_datagen.flow_from_dataframe(validation_data_specialized,\n",
    "                                                    directory=os.path.join(data_root),\n",
    "                                                    x_col=\"image_path\",\n",
    "                                                    y_col=\"item_category\",\n",
    "                                                    target_size=IMAGE_SIZE,\n",
    "                                                    color_mode=\"rgb\",\n",
    "                                                    class_mode=\"categorical\",\n",
    "                                                    shuffle=True,\n",
    "                                                    batch_size=64,\n",
    "                                                    )\n",
    "\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(test_data_specialized,\n",
    "                                                  directory=os.path.join(data_root),\n",
    "                                                  x_col=\"image_path\",\n",
    "                                                  y_col=None,\n",
    "                                                  target_size=IMAGE_SIZE,\n",
    "                                                  color_mode=\"rgb\",\n",
    "                                                  class_mode=None,\n",
    "                                                  shuffle=False,\n",
    "                                                  batch_size=64,\n",
    "                                                  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "418/418 [==============================] - 155s 370ms/step - loss: 3.4827 - acc: 0.1070 - val_loss: 2.5526 - val_acc: 0.1536\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.15365, saving model to ../checkpoints/epoch_10_03_19_2019_00_19_17v2.hdf5\n",
      "Epoch 2/10\n",
      "418/418 [==============================] - 136s 326ms/step - loss: 2.3451 - acc: 0.2439 - val_loss: 2.4598 - val_acc: 0.1831\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.15365 to 0.18309, saving model to ../checkpoints/epoch_10_03_19_2019_00_19_17v2.hdf5\n",
      "Epoch 3/10\n",
      "418/418 [==============================] - 138s 330ms/step - loss: 1.7592 - acc: 0.4485 - val_loss: 2.5097 - val_acc: 0.1929\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.18309 to 0.19290, saving model to ../checkpoints/epoch_10_03_19_2019_00_19_17v2.hdf5\n",
      "Epoch 4/10\n",
      "418/418 [==============================] - 138s 330ms/step - loss: 1.1716 - acc: 0.6520 - val_loss: 2.6490 - val_acc: 0.1937\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.19290 to 0.19366, saving model to ../checkpoints/epoch_10_03_19_2019_00_19_17v2.hdf5\n",
      "Epoch 5/10\n",
      "418/418 [==============================] - 136s 325ms/step - loss: 0.8101 - acc: 0.7689 - val_loss: 2.9479 - val_acc: 0.2039\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.19366 to 0.20385, saving model to ../checkpoints/epoch_10_03_19_2019_00_19_17v2.hdf5\n",
      "Epoch 6/10\n",
      "275/418 [==================>...........] - ETA: 43s - loss: 0.6064 - acc: 0.8347"
     ]
    }
   ],
   "source": [
    "def gen_filename_h5():\n",
    "    return 'epoch_'+str(epochs) + '_' + datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "\n",
    "\n",
    "def gen_filename_csv():\n",
    "    return 'epoch_'+str(epochs) + '_' + datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "\n",
    "\n",
    "# Checkpoint auto\n",
    "filepath = \"../checkpoints/\"+gen_filename_h5()+\"v2.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "\n",
    "steps_per_epoch = image_generator.samples//image_generator.batch_size\n",
    "valid_steps_per_epoch = valid_generator.samples // valid_generator.batch_size\n",
    "test_steps_per_epoch = test_generator.samples // test_generator.batch_size\n",
    "\n",
    "history = model.fit_generator(generator=image_generator,\n",
    "                              steps_per_epoch=steps_per_epoch,\n",
    "                              validation_data=valid_generator,\n",
    "                              validation_steps=valid_steps_per_epoch,\n",
    "                              epochs=epochs,\n",
    "                              callbacks=[checkpointer],\n",
    "                              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_test():\n",
    "    prediction_specialized = model.predict_generator(test_generator, verbose=1, steps=test_steps_per_epoch)\n",
    "    predicted_label_specialized = [np.argmax(prediction_specialized[i]) for i in range(len(prediction_specialized))]\n",
    "    df = pd.DataFrame({'itemid': test_data_specialized['itemid'].astype(int), 'Category': predicted_label_specialized})\n",
    "    df.to_csv(path_or_buf='res' + gen_filename_csv() + '.csv', index=False)\n",
    "\n",
    "if gen_test:\n",
    "    perform_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
