{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "from keras import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Lambda, Dense\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utility.train_data_loader import load_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom train data used\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasonlimantoro/.virtualenvs/ndsc_beginner/lib/python3.6/site-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 256\n",
    "specialization = \"mobile\"\n",
    "gen_test = False\n",
    "\n",
    "categories_file = open(\"../data/categories.json\", \"r\")\n",
    "categories = json.load(categories_file)\n",
    "\n",
    "all_subcategories = {k.lower(): v for k, v in categories['Mobile'].items()}\n",
    "all_subcategories.update({k.lower(): v for k, v in categories['Fashion'].items()})\n",
    "all_subcategories.update({k.lower(): v for k, v in categories['Beauty'].items()})\n",
    "\n",
    "data_root = \"../../\"+specialization+\"_image/\"\n",
    "\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "valid_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "feature_extractor_url = \"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/2\"\n",
    "\n",
    "trainData = load_train_data()\n",
    "testData = pd.read_csv(\"../data/test.csv\")\n",
    "\n",
    "validation_data_specialized = trainData[trainData['image_path'].str.contains(specialization)][::100]\n",
    "validation_data_specialized['image_path'] = validation_data_specialized['image_path']. \\\n",
    "    map(lambda x: x.replace(specialization + '_image/', ''))\n",
    "\n",
    "test_data_specialized = testData[testData['image_path'].str.contains(specialization)]\n",
    "test_data_specialized['image_path'] = test_data_specialized['image_path'].\\\n",
    "    map(lambda x: x.replace(specialization+'_image/', ''))\n",
    "\n",
    "inverted_categories_specialized = {k.lower(): v for k, v in categories[specialization.capitalize()].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasonlimantoro/.virtualenvs/ndsc_beginner/lib/python3.6/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "train_data_specialized = trainData[trainData['image_path'].str.contains(specialization)][::]\n",
    "df = pd.DataFrame()\n",
    "for k,v in inverted_categories_specialized.items():\n",
    "    rows = train_data_specialized.loc[train_data_specialized['Category'] == v][:100]\n",
    "    df = df.append(rows)\n",
    "\n",
    "train_data_specialized = df\n",
    "\n",
    "train_data_specialized['image_path'] = train_data_specialized['image_path']. \\\n",
    "    map(lambda x: x.replace(specialization + '_image/', ''))\n",
    "\n",
    "\n",
    "validation_data_specialized = train_data_specialized[::10]\n",
    "validation_data_specialized['image_path'] = validation_data_specialized['image_path']. \\\n",
    "    map(lambda x: x.replace(specialization + '_image/', ''))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using /var/folders/pq/g3h8qm9x6730w715x51yqq_r0000gp/T/tfhub_modules to cache modules.\n",
      "WARNING:tensorflow:From /Users/jasonlimantoro/.virtualenvs/ndsc_beginner/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Found 2648 images belonging to 27 classes.\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = hub.get_expected_image_size(hub.Module(feature_extractor_url))\n",
    "image_generator = datagen.flow_from_dataframe(train_data_specialized,\n",
    "                                              directory=os.path.join(data_root),\n",
    "                                              x_col=\"image_path\",\n",
    "                                              y_col=\"item_category\",\n",
    "                                              target_size=IMAGE_SIZE,\n",
    "                                              color_mode=\"rgb\",\n",
    "                                              class_mode=\"categorical\",\n",
    "                                              shuffle=True,\n",
    "                                              batch_size=64,\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch shape:  (64, 224, 224, 3)\n",
      "Label batch shape:  (64, 27)\n"
     ]
    }
   ],
   "source": [
    "label_names = sorted(image_generator.class_indices.items(), key=lambda pair:pair[1])\n",
    "label_names = np.array([key.title() for key, value in label_names])\n",
    "\n",
    "\n",
    "def feature_extractor(x):\n",
    "    feature_extractor_module = hub.Module(feature_extractor_url)\n",
    "    return feature_extractor_module(x)\n",
    "\n",
    "\n",
    "for image_batch, label_batch in image_generator:\n",
    "    print(\"Image batch shape: \", image_batch.shape)\n",
    "    print(\"Label batch shape: \", label_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_1 (Lambda)            (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 27)                34587     \n",
      "=================================================================\n",
      "Total params: 34,587\n",
      "Trainable params: 34,587\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Lambda(feature_extractor, input_shape=IMAGE_SIZE+[3], trainable=True))\n",
    "model.add(Dense(len(inverted_categories_specialized), activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 265 images belonging to 27 classes.\n",
      "Found 40417 images.\n"
     ]
    }
   ],
   "source": [
    "valid_generator = valid_datagen.flow_from_dataframe(validation_data_specialized,\n",
    "                                                    directory=os.path.join(data_root),\n",
    "                                                    x_col=\"image_path\",\n",
    "                                                    y_col=\"item_category\",\n",
    "                                                    target_size=IMAGE_SIZE,\n",
    "                                                    color_mode=\"rgb\",\n",
    "                                                    class_mode=\"categorical\",\n",
    "                                                    shuffle=True,\n",
    "                                                    batch_size=64,\n",
    "                                                    )\n",
    "\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(test_data_specialized,\n",
    "                                                  directory=os.path.join(data_root),\n",
    "                                                  x_col=\"image_path\",\n",
    "                                                  y_col=None,\n",
    "                                                  target_size=IMAGE_SIZE,\n",
    "                                                  color_mode=\"rgb\",\n",
    "                                                  class_mode=None,\n",
    "                                                  shuffle=False,\n",
    "                                                  batch_size=64,\n",
    "                                                  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "41/41 [==============================] - 97s 2s/step - loss: 2.5640 - acc: 0.2888 - val_loss: 2.4007 - val_acc: 0.3164\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.31641, saving model to ../checkpoints/epoch_10_03_18_2019_03_01_42v2.hdf5\n",
      "Epoch 2/10\n",
      "41/41 [==============================] - 90s 2s/step - loss: 2.3001 - acc: 0.3675 - val_loss: 2.1403 - val_acc: 0.4030\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.31641 to 0.40299, saving model to ../checkpoints/epoch_10_03_18_2019_03_01_42v2.hdf5\n",
      "Epoch 3/10\n",
      "41/41 [==============================] - 89s 2s/step - loss: 2.0663 - acc: 0.4333 - val_loss: 2.0617 - val_acc: 0.4229\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.40299 to 0.42289, saving model to ../checkpoints/epoch_10_03_18_2019_03_01_42v2.hdf5\n",
      "Epoch 4/10\n",
      "41/41 [==============================] - 103s 3s/step - loss: 1.8905 - acc: 0.4923 - val_loss: 1.7801 - val_acc: 0.5373\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.42289 to 0.53731, saving model to ../checkpoints/epoch_10_03_18_2019_03_01_42v2.hdf5\n",
      "Epoch 5/10\n",
      "41/41 [==============================] - 91s 2s/step - loss: 1.7581 - acc: 0.5353 - val_loss: 1.5878 - val_acc: 0.6269\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.53731 to 0.62687, saving model to ../checkpoints/epoch_10_03_18_2019_03_01_42v2.hdf5\n",
      "Epoch 6/10\n",
      "41/41 [==============================] - 92s 2s/step - loss: 1.6276 - acc: 0.5873 - val_loss: 1.5434 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.62687\n",
      "Epoch 7/10\n",
      "41/41 [==============================] - 86s 2s/step - loss: 1.5196 - acc: 0.6270 - val_loss: 1.4619 - val_acc: 0.6617\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.62687 to 0.66169, saving model to ../checkpoints/epoch_10_03_18_2019_03_01_42v2.hdf5\n",
      "Epoch 8/10\n",
      "41/41 [==============================] - 85s 2s/step - loss: 1.4347 - acc: 0.6443 - val_loss: 1.3229 - val_acc: 0.6567\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.66169\n",
      "Epoch 9/10\n",
      "41/41 [==============================] - 88s 2s/step - loss: 1.3248 - acc: 0.6849 - val_loss: 1.3114 - val_acc: 0.7264\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.66169 to 0.72637, saving model to ../checkpoints/epoch_10_03_18_2019_03_01_42v2.hdf5\n",
      "Epoch 10/10\n",
      "41/41 [==============================] - 84s 2s/step - loss: 1.2751 - acc: 0.7064 - val_loss: 1.2421 - val_acc: 0.7114\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.72637\n"
     ]
    }
   ],
   "source": [
    "def gen_filename_h5():\n",
    "    return 'epoch_'+str(epochs) + '_' + datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "\n",
    "\n",
    "def gen_filename_csv():\n",
    "    return 'epoch_'+str(epochs) + '_' + datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "\n",
    "\n",
    "# Checkpoint auto\n",
    "filepath = \"../checkpoints/\"+gen_filename_h5()+\"v2.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "\n",
    "steps_per_epoch = image_generator.samples//image_generator.batch_size\n",
    "valid_steps_per_epoch = valid_generator.samples // valid_generator.batch_size\n",
    "test_steps_per_epoch = test_generator.samples // test_generator.batch_size\n",
    "\n",
    "history = model.fit_generator(generator=image_generator,\n",
    "                              steps_per_epoch=steps_per_epoch,\n",
    "                              validation_data=valid_generator,\n",
    "                              validation_steps=valid_steps_per_epoch,\n",
    "                              epochs=epochs,\n",
    "                              callbacks=[checkpointer],\n",
    "                              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
